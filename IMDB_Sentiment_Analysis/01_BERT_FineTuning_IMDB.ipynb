{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pTNOOd0yNU9"
   },
   "outputs": [],
   "source": [
    "#installing while silencing the outputs\n",
    "\n",
    "pip install transformers > NUL 2>&1"
   ],
   "id": "6pTNOOd0yNU9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c9d64ac"
   },
   "outputs": [],
   "source": [
    "#importing all the essential libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import re #for data pre-processing\n",
    "import unicodedata #library for data pre-processing\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ],
   "id": "2c9d64ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8wckJC6jhOZ",
    "outputId": "45a2201f-231d-43e5-fa47-73af4121f770"
   },
   "outputs": [],
   "source": [
    "#mounting drive to collab\n",
    "drive.mount('/content/drive')"
   ],
   "id": "S8wckJC6jhOZ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXCbexQYjff8"
   },
   "outputs": [],
   "source": [
    "#Reading file from drive\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IMDB_Dataset.csv')"
   ],
   "id": "HXCbexQYjff8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9a90d8c5",
    "outputId": "037c60e2-26e6-42a5-fbb3-aa0839e2c2dd"
   },
   "outputs": [],
   "source": [
    "#Printing top 5 rows\n",
    "df.head()"
   ],
   "id": "9a90d8c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8e79f07",
    "outputId": "3665eb4e-691c-49d0-e28b-a3e828d8e656"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "id": "d8e79f07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "751db7a6"
   },
   "outputs": [],
   "source": [
    "#Assigning X & y with review and sentiment respectively\n",
    "\n",
    "X = df['review']\n",
    "y = df['sentiment']"
   ],
   "id": "751db7a6"
  },
  {
   "cell_type": "code",
   "source": [
    "#splitting values in to train, validation and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.7,stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.57, random_state=42)"
   ],
   "metadata": {
    "id": "zg6tiVgi6mfx"
   },
   "id": "zg6tiVgi6mfx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa8a27d9",
    "outputId": "59bc5119-7120-4389-f2b6-72123605eebd"
   },
   "outputs": [],
   "source": [
    "#chceking the length of the train, validation and test sets\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))\n",
    "print(\"Validation set size:\", len(X_val))"
   ],
   "id": "fa8a27d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c16cb1a2",
    "outputId": "f5bbd9a0-63c4-48a9-c148-d653b6c470bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#seeing the index locations for the variables\n",
    "\n",
    "print(X_train.index)\n",
    "print(X_test.index)\n",
    "print(X_val.index)"
   ],
   "id": "c16cb1a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e6f1bc5"
   },
   "outputs": [],
   "source": [
    "#resetting the index locations\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)"
   ],
   "id": "6e6f1bc5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb865597",
    "outputId": "417859c8-ddc3-4154-e31b-58c40b0c68bd"
   },
   "outputs": [],
   "source": [
    "# Verifying the updated index values\n",
    "\n",
    "print(X_train.index)\n",
    "print(X_test.index)\n",
    "print(X_val.index)"
   ],
   "id": "cb865597"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d46f9da",
    "outputId": "fd1934b1-d2d7-439b-d7bc-610afc8f0314"
   },
   "outputs": [],
   "source": [
    "#Printing the data brfore pre processing\n",
    "\n",
    "print(X_train[0])\n",
    "print(X_test[0])\n",
    "print(X_val[0])"
   ],
   "id": "6d46f9da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6ed3979",
    "outputId": "f9cd561c-63c5-4985-f416-458646eff184"
   },
   "outputs": [],
   "source": [
    "#seeing the index locations for the variables\n",
    "\n",
    "print(y_train.index)\n",
    "print(y_test.index)\n",
    "print(y_val.index)"
   ],
   "id": "a6ed3979"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa429b55"
   },
   "outputs": [],
   "source": [
    "#resetting the index locations\n",
    "\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)"
   ],
   "id": "aa429b55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "266952ff",
    "outputId": "c3ebf183-7c26-42a6-be36-5c183ddd19d6"
   },
   "outputs": [],
   "source": [
    "print(y_train.index)\n",
    "print(y_test.index)\n",
    "print(y_val.index)"
   ],
   "id": "266952ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35190789",
    "outputId": "2a060751-918e-4d57-878c-2f0496ed96aa"
   },
   "outputs": [],
   "source": [
    "print(y_train[0])\n",
    "print(y_test[0])\n",
    "print(y_val[0])"
   ],
   "id": "35190789"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfzB5KuXRWf-"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Removing HTML tags\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "\n",
    "    # Removing accented characters by normalizing to ASCII\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    # Replacing patterns like n't, nt, and n t with not\n",
    "    text = re.sub(r\"\\b(n['\u2019]?t|n\\s?t)\\b\", ' not', text)\n",
    "\n",
    "    # Removing special characters\n",
    "    text = re.sub(\"[^a-zA-Z0-9,'!]\", ' ', text)\n",
    "\n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Separating text into sentences based on punctuations and then removing punctuations\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [re.sub('[,]', '', sentence) for sentence in sentences]\n",
    "\n",
    "    # Removing extra spaces\n",
    "    text = ' '.join(' '.join(sentence.split()) for sentence in sentences)\n",
    "\n",
    "    return text"
   ],
   "id": "WfzB5KuXRWf-"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc48330c"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "X_train_preprocessed = [preprocess_text(text) for text in X_train]\n",
    "\n",
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
    "\n",
    "X_val_preprocessed = [preprocess_text(text) for text in X_val]\n"
   ],
   "id": "dc48330c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "244e45ff",
    "outputId": "a21e2603-962f-4b9a-ab59-9c772c50502e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Printing pre processed data\n",
    "\n",
    "print(X_train_preprocessed[0])\n",
    "print(X_test_preprocessed[0])\n",
    "print(X_val_preprocessed[0])"
   ],
   "id": "244e45ff"
  },
  {
   "cell_type": "code",
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "y_train_preprocessed = [preprocess_text(text) for text in y_train]\n",
    "\n",
    "y_test_preprocessed = [preprocess_text(text) for text in y_test]\n",
    "\n",
    "y_val_preprocessed = [preprocess_text(text) for text in y_val]\n"
   ],
   "metadata": {
    "id": "b5A_WZqOoQlR"
   },
   "execution_count": null,
   "outputs": [],
   "id": "b5A_WZqOoQlR"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tokenizing and creating datasets for train, validation and test sets so they can be used through out different types of training sessions and evaluation of diferent models"
   ],
   "metadata": {
    "id": "g8cgsBICNtdQ"
   },
   "id": "g8cgsBICNtdQ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e9c29f9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "id": "1e9c29f9"
  },
  {
   "cell_type": "code",
   "source": [
    "# Tokenizing and encoding the train data\n",
    "X_train_encoded = tokenizer.batch_encode_plus(\n",
    "    X_train_preprocessed,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "# Tokenizing and encoding the validation data\n",
    "X_val_encoded = tokenizer.batch_encode_plus(\n",
    "    X_val_preprocessed,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")"
   ],
   "metadata": {
    "id": "7H9Kbj8altls"
   },
   "id": "7H9Kbj8altls",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "label_map = {\"positive\": 1, \"negative\": 0}\n",
    "\n",
    "# Extracting input tensors from the encoded data\n",
    "input_ids_train = X_train_encoded['input_ids']\n",
    "attention_masks_train = X_train_encoded['attention_mask']\n",
    "labels_train = torch.tensor([label_map[label] for label in y_train_preprocessed])\n",
    "\n",
    "input_ids_val = X_val_encoded['input_ids']\n",
    "attention_masks_val = X_val_encoded['attention_mask']\n",
    "labels_val = torch.tensor([label_map[label] for label in y_val_preprocessed])\n",
    "\n",
    "# Creating datasets\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ],
   "metadata": {
    "id": "2UHfvfiFlycP"
   },
   "id": "2UHfvfiFlycP",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Tokenizing and encoding the test data\n",
    "X_test_encoded = tokenizer.batch_encode_plus(\n",
    "    X_test_preprocessed,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")"
   ],
   "metadata": {
    "id": "W9ilC5yKNfx2"
   },
   "execution_count": null,
   "outputs": [],
   "id": "W9ilC5yKNfx2"
  },
  {
   "cell_type": "code",
   "source": [
    "# Extracting input tensors and attention masks from the encoded data\n",
    "label_map = {\"positive\": 1, \"negative\": 0}\n",
    "\n",
    "input_ids_test = X_test_encoded['input_ids']\n",
    "attention_masks_test = X_test_encoded['attention_mask']\n",
    "labels_test = torch.tensor([label_map[label] for label in y_test_preprocessed])\n",
    "\n",
    "# Creating datasets\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
   ],
   "metadata": {
    "id": "2NqSgZcGNfx2"
   },
   "execution_count": null,
   "outputs": [],
   "id": "2NqSgZcGNfx2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32035a33",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "32035a33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Optimization using Hidden Layers\n"
   ],
   "metadata": {
    "id": "JXBiE9RKO_8E"
   },
   "id": "JXBiE9RKO_8E"
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_bert_layers=12):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Learnable weights for each layer\n",
    "        self.layer_weights = nn.Parameter(torch.ones(num_bert_layers) / num_bert_layers)\n",
    "\n",
    "        # New layers\n",
    "        self.linear1 = nn.Linear(768, 256)\n",
    "        self.linear2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get all hidden states\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        all_hidden_states = outputs[2]\n",
    "\n",
    "        # Weighted combination of all layers\n",
    "        weighted_sum = sum(w * hidden_state for w, hidden_state in zip(self.layer_weights, all_hidden_states))\n",
    "\n",
    "        linear1_output = self.linear1(weighted_sum[:,0,:].view(-1,768))\n",
    "        linear2_output = self.linear2(linear1_output).squeeze()\n",
    "\n",
    "\n",
    "        return linear2_output"
   ],
   "metadata": {
    "id": "iS-HnuV0GAGC"
   },
   "id": "iS-HnuV0GAGC",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = CustomBERTModel()"
   ],
   "metadata": {
    "id": "9g8-hwNWGAI9"
   },
   "id": "9g8-hwNWGAI9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn import CrossEntropyLoss"
   ],
   "metadata": {
    "id": "NATaTCKXux7H"
   },
   "id": "NATaTCKXux7H",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Moving the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# declaring Hyperparameters\n",
    "batch_size = 32\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "stop_training = 0\n",
    "num_cpu = 12\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "train_losses.clear()\n",
    "val_accuracies.clear()\n",
    "\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "lambda_lr = lambda epoch: 0.95 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Creating data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for inputs, attention_masks, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "# Validation\n",
    "    model.eval()  # Setting model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model's predictions\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            stop_training = 1\n",
    "            break\n",
    "\n",
    "    # Printing the loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Saving the trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBHLO1.pt\"\n",
    "torch.save(best_model_state, BERT_Tuned_file)"
   ],
   "metadata": {
    "id": "eyTpftl4GAMg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6109263b-ec58-42af-d529-274bc79f5173"
   },
   "id": "eyTpftl4GAMg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting epoch loss and validation accuracy graphs\n",
    "num_epochs = len(train_losses)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/BBHLO1_plot.png\"\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "4urNufgWP_bn",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "outputId": "45a1cf9c-88d0-48f5-a1d5-13948b1ef41b"
   },
   "execution_count": null,
   "outputs": [],
   "id": "4urNufgWP_bn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8785cbb4-5198-4752-96d5-dcc18c06b408",
    "id": "884uqZ0PjMJg"
   },
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBHLO1.pt\"\n",
    "model.load_state_dict(torch.load(BERT_Tuned_file))"
   ],
   "id": "884uqZ0PjMJg"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "36e3a6c7-4ece-43cb-c32f-17ba81ce485c",
    "id": "xnz-qlv4jMJh"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Creating dataloaders for test\n",
    "batch_size = 32\n",
    "num_cpu = 12\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "all_preds.clear()\n",
    "all_labels.clear()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ],
   "id": "xnz-qlv4jMJh"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Tezting the Pre-Training of BERT for better accuracy!"
   ],
   "metadata": {
    "id": "8HkPt80YoX9g"
   },
   "id": "8HkPt80YoX9g"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import copy"
   ],
   "metadata": {
    "id": "teDeJ47GUfey"
   },
   "id": "teDeJ47GUfey",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Data Pre-processing\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/wikitext-2/wiki.train.tokens', 'r') as file:\n",
    "    wikitext_data = file.readlines()\n",
    "\n",
    "pretrain_preprocessed = [preprocess_text(text) for text in wikitext_data]\n",
    "\n",
    "pretrain_encoded = tokenizer.batch_encode_plus(\n",
    "    pretrain_preprocessed,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "pretrain_dataset = pretrain_encoded\n",
    "\n",
    "# Data Pre-processing for validation data\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/wikitext-2/wiki.valid.tokens', 'r') as file:\n",
    "    wikitext_val_data = file.readlines()\n",
    "\n",
    "preval_preprocessed = [preprocess_text(text) for text in wikitext_val_data]\n",
    "\n",
    "# Encoding the validation data\n",
    "preval_encoded = tokenizer.batch_encode_plus(\n",
    "    preval_preprocessed,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "preval_dataset = preval_encoded"
   ],
   "metadata": {
    "id": "OXZlfBz7wHEL"
   },
   "id": "OXZlfBz7wHEL",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pretrain_dataset_list = [{k: v[i] for k, v in pretrain_encoded.items()} for i in range(len(pretrain_encoded['input_ids']))]\n",
    "preval_dataset_list = [{k: v[i] for k, v in preval_encoded.items()} for i in range(len(preval_encoded['input_ids']))]"
   ],
   "metadata": {
    "id": "KUWgNBnU5ZBi"
   },
   "id": "KUWgNBnU5ZBi",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)"
   ],
   "metadata": {
    "id": "5uG2-cC7oUO5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149,
     "referenced_widgets": [
      "66b47aafb57949d793b470ed816cd405",
      "dc6e906e176946669281ef9ca4a32562",
      "b13161aab05347a4a8a8fb9840e772ca",
      "9dcb9e4253744b75a0e52c679c07863f",
      "1ab72a3125b54109b4a60903e08c3240",
      "544e3dd7ed164d36bac8f7c20670d103",
      "eef0915666b74465a9b78ddd0d849d7b",
      "cc206d92fd794419ab80d569dd8713cd",
      "f460075cf42e41bfb17b1ddfd2fb283f",
      "7063fd0e3e744e2cb43d45cd310e2fc9",
      "fdd966b9c1934687b3d577723ad5fa8d"
     ]
    },
    "outputId": "b64bc871-a203-4be7-fcca-0d1dd29ccf4d"
   },
   "id": "5uG2-cC7oUO5",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 16\n",
    "num_cpu = 12\n",
    "num_epochs_pretrain = 3\n",
    "early_stopping_patience = 2\n",
    "epochs_without_improvement = 0\n",
    "best_accuracy = 0.0\n",
    "pre_train_losses = []\n",
    "pre_train_accuracies = []\n",
    "pre_train_losses.clear()\n",
    "pre_train_accuracies.clear()\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "pretrain_dataloader = DataLoader(pretrain_dataset_list, batch_size=batch_size, num_workers=num_cpu, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(preval_dataset_list, batch_size=batch_size, num_workers=num_cpu, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Pre-training\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs_pretrain, eta_min=1e-6)\n",
    "\n",
    "for epoch in range(num_epochs_pretrain):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch in pretrain_dataloader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_masks = batch['attention_mask'].to(device)\n",
    "        labels = inputs.clone()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    pre_train_losses.append(epoch_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pretrain_dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_masks = batch['attention_mask'].to(device)\n",
    "            labels = inputs.clone()\n",
    "\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predicted_indices = torch.argmax(logits, 2)\n",
    "            valid_positions = (labels != tokenizer.pad_token_id) & (labels != tokenizer.cls_token_id) & (labels != tokenizer.sep_token_id)\n",
    "            valid_predictions = (predicted_indices == labels) & valid_positions\n",
    "            correct_predictions += valid_predictions.sum().item()\n",
    "            total_predictions += valid_positions.sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    pre_train_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        best_model = copy.deepcopy(model)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Printing the loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_pretrain}, Loss: {epoch_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Saving the trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/bert_pretrained\"\n",
    "best_model.save_pretrained(BERT_Tuned_file)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4L0TU675cVTn",
    "outputId": "543838cf-4bda-4890-cf4b-807a77b5c698"
   },
   "execution_count": null,
   "outputs": [],
   "id": "4L0TU675cVTn"
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting epoch loss and validation accuracy graphs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "num_epochs_pretrain = len(pre_train_losses)\n",
    "plt.plot(range(1, num_epochs_pretrain + 1), pre_train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs_pretrain + 1), pre_train_accuracies, label='Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/BBPre_Training_plot.png\"\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "USDfgtRGHpPj",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "outputId": "b70d1503-bd77-407c-e9f1-e01a994811db"
   },
   "id": "USDfgtRGHpPj",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/bert_pretrained\", num_labels=2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eh1I9JZ4Uf5v",
    "outputId": "f73ca9ca-fc3d-45a6-c93e-ffc4022218c4"
   },
   "id": "Eh1I9JZ4Uf5v",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "# declaring Hyperparameters\n",
    "batch_size = 32\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "num_cpu = 12\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "train_losses.clear()\n",
    "val_accuracies.clear()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "# Creating data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for inputs, attention_masks, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks, labels =labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "   # Validation\n",
    "    model.eval()  # Setting model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model's predictions\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Printing the loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Saving the trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBPT.pt\"\n",
    "torch.save(best_model_state, BERT_Tuned_file)"
   ],
   "metadata": {
    "id": "AgcnOwKjUfnz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f6cba12f-00ed-48ca-ff0f-d51c074ac4b1"
   },
   "id": "AgcnOwKjUfnz",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting epoch loss and validation accuracy graphs\n",
    "num_epochs = len(train_losses)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/BBPT_plot.png\"\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "2KZosjcoJTes",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "outputId": "83cdc66b-19da-4776-e786-12714d62a306"
   },
   "execution_count": null,
   "outputs": [],
   "id": "2KZosjcoJTes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e12ceab1-36c4-49d4-9bae-f2d9aa7b2f2b",
    "id": "lK9YpspUJTet"
   },
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBPT.pt\"\n",
    "model.load_state_dict(torch.load(BERT_Tuned_file))"
   ],
   "id": "lK9YpspUJTet"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5650a0b9-1d7a-4e37-b171-7fdd75803b96",
    "id": "jqi7pyq7JTeu"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Creating dataloaders for test\n",
    "batch_size = 32\n",
    "num_cpu = 12\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "all_preds.clear()\n",
    "all_labels.clear()\n",
    "\n",
    "with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ],
   "id": "jqi7pyq7JTeu"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Trying Distil BERT"
   ],
   "metadata": {
    "id": "ThJoyIrk2JOQ"
   },
   "id": "ThJoyIrk2JOQ"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "id": "qiMikE0OpzIK"
   },
   "id": "qiMikE0OpzIK",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "num_cpu = 12\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "temperature = 5.0\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "train_losses.clear()\n",
    "val_accuracies.clear()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initializing the BERT teacher model\n",
    "teacher_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2).to(device)\n",
    "teacher_model.eval()  # Setting to evaluation mode\n",
    "\n",
    "# Initializing the DistilBERT student model and tokenizer\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=2e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "# Creating data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    student_model.train()  # Setting to training mode\n",
    "\n",
    "    for inputs, attention_masks, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs, attention_mask=attention_masks)\n",
    "            teacher_logits =  teacher_outputs.logits\n",
    "\n",
    "        student_outputs = student_model(inputs, attention_mask=attention_masks)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        soft_loss = loss_fn((student_logits / temperature), (teacher_logits / temperature))\n",
    "        hard_loss = loss_fn(student_logits, labels)\n",
    "        loss = 0.75 * soft_loss + 0.25 * hard_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Validation\n",
    "    student_model.eval()  # Setting model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model's predictions\n",
    "            student_outputs = student_model(inputs, attention_mask=attention_masks)\n",
    "            predicted_labels = torch.argmax(student_outputs.logits, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        best_model_state = student_model.state_dict()\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Printing the loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Saving the trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/DB1-1.pt\"\n",
    "torch.save(best_model_state, BERT_Tuned_file)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f5483d58-4789-4a0b-cf22-6b954cf7af28",
    "id": "iTFlOttX-iGZ"
   },
   "execution_count": null,
   "outputs": [],
   "id": "iTFlOttX-iGZ"
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting epoch loss and validation accuracy graphs\n",
    "num_epochs = len(train_losses)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/DB1-1_plot.png\"\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "outputId": "4601e58e-2173-4705-b548-b2f2773a9d24",
    "id": "bG1Zrr68-iGa"
   },
   "execution_count": null,
   "outputs": [],
   "id": "bG1Zrr68-iGa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoxrfbit-iGa"
   },
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/DB1-1.pt\"\n",
    "student_model.load_state_dict(torch.load(BERT_Tuned_file))"
   ],
   "id": "xoxrfbit-iGa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXGlchfz-iGa",
    "outputId": "25743a3f-ee3e-4cb3-9f85-1a5a777d2cba"
   },
   "outputs": [],
   "source": [
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "# Creating dataloaders for test\n",
    "batch_size = 32\n",
    "num_cpu = 12\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "all_preds.clear()\n",
    "all_labels.clear()\n",
    "\n",
    "with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            student_outputs = student_model(inputs, attention_mask=attention_masks)\n",
    "            _, preds = torch.max(student_outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ],
   "id": "uXGlchfz-iGa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1 Trying Distil BERT with Kullback-Leibler Loss function"
   ],
   "metadata": {
    "id": "wI9s2uSJNVIk"
   },
   "id": "wI9s2uSJNVIk"
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "num_cpu = 12\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "temperature = 5.0\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "train_losses.clear()\n",
    "val_accuracies.clear()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initializing the BERT teacher model\n",
    "teacher_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2).to(device)\n",
    "teacher_model.eval()  # Setting to evaluation mode\n",
    "\n",
    "# Initializing the DistilBERT student model and tokenizer\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=2e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "# Creating data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    student_model.train()  # Setting to training mode\n",
    "\n",
    "    for inputs, attention_masks, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs, attention_mask=attention_masks)\n",
    "            teacher_logits =  teacher_outputs.logits\n",
    "\n",
    "        student_outputs = student_model(inputs, attention_mask=attention_masks)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        soft_loss = F.kl_div(F.log_softmax(student_logits / temperature, dim=1), F.softmax(teacher_logits / temperature, dim=1), reduction='batchmean')\n",
    "\n",
    "        hard_loss = loss_fn(student_logits, labels)\n",
    "        loss = 0.75 * soft_loss + 0.25 * hard_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Validation\n",
    "    student_model.eval()  # Setting model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model's predictions\n",
    "            student_outputs = student_model(inputs, attention_mask=attention_masks)\n",
    "            predicted_labels = torch.argmax(student_outputs.logits, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        best_model_state = student_model.state_dict()\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Printing the loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Saving the trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/DB2.pt\"\n",
    "torch.save(best_model_state, BERT_Tuned_file)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "54c1886b-1eb6-4ba2-f5dd-ecf05e3dc036",
    "id": "7XgxYkgGNVIl"
   },
   "execution_count": null,
   "outputs": [],
   "id": "7XgxYkgGNVIl"
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting epoch loss and validation accuracy graphs\n",
    "num_epochs = len(train_losses)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/DB2_plot.png\"\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "bZjyhyS8NVIl",
    "outputId": "4189c13d-9580-4ac8-a2a0-1af2c4e1d665"
   },
   "execution_count": null,
   "outputs": [],
   "id": "bZjyhyS8NVIl"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlVkvb8SNVIm"
   },
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/DB2.pt\"\n",
    "student_model.load_state_dict(torch.load(BERT_Tuned_file))"
   ],
   "id": "VlVkvb8SNVIm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e4f97c1f-b7e4-4a09-e00a-17256d3aad2c",
    "id": "VyuWBkBXNVIm"
   },
   "outputs": [],
   "source": [
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "# Creating dataloaders for test\n",
    "batch_size = 32\n",
    "num_cpu = 12\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "all_preds.clear()\n",
    "all_labels.clear()\n",
    "\n",
    "with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            student_outputs = student_model(inputs, attention_mask=attention_masks)\n",
    "            _, preds = torch.max(student_outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ],
   "id": "VyuWBkBXNVIm"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1.1 Trying Distil BERT with Kullback-Leibler Loss function adjusting the weights"
   ],
   "metadata": {
    "id": "DI96ff5i2dK-"
   },
   "id": "DI96ff5i2dK-"
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "num_cpu = 12\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "temperature = 5.0\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "train_losses.clear()\n",
    "val_accuracies.clear()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initializing the BERT teacher model\n",
    "teacher_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2).to(device)\n",
    "teacher_model.eval()  # Setting to evaluation mode\n",
    "\n",
    "# Initializing the DistilBERT student model and tokenizer\n",
    "student_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels = 2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=2e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "# Creating data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    student_model.train()  # Setting to training mode\n",
    "\n",
    "    for inputs, attention_masks, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs, attention_mask=attention_masks)\n",
    "            teacher_logits =  teacher_outputs.logits\n",
    "\n",
    "        student_outputs = student_model(inputs, attention_mask=attention_masks)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        soft_loss = F.kl_div(F.log_softmax(student_logits / temperature, dim=1), F.softmax(teacher_logits / temperature, dim=1), reduction='batchmean')\n",
    "\n",
    "        hard_loss = loss_fn(student_logits, labels)\n",
    "        loss = 0.95 * soft_loss + 0.05 * hard_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# Validation\n",
    "    student_model.eval()  # Setting model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model's predictions\n",
    "            student_outputs = student_model(inputs, attention_mask=attention_masks)\n",
    "            predicted_labels = torch.argmax(student_outputs.logits, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        best_model_state = student_model.state_dict()\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Printing the loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Saving the trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/DB2-1.pt\"\n",
    "torch.save(best_model_state, BERT_Tuned_file)"
   ],
   "metadata": {
    "id": "gukGmlGL2abm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "266ca0b5-3e6d-45b6-968d-e500b3820f8d"
   },
   "execution_count": null,
   "outputs": [],
   "id": "gukGmlGL2abm"
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting epoch loss and validation accuracy graphs\n",
    "num_epochs = len(train_losses)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/DB2-1_plot.png\"\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "v5cTS1AT2abo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "outputId": "2375af6d-e387-43be-b82d-ae2ee7cd58a2"
   },
   "execution_count": null,
   "outputs": [],
   "id": "v5cTS1AT2abo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXejLQkh2abq"
   },
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/DB2-1.pt\"\n",
    "student_model.load_state_dict(torch.load(BERT_Tuned_file))"
   ],
   "id": "AXejLQkh2abq"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6881ab8c-e092-4eb2-da2b-7cadde3cecee",
    "id": "RY1HWfx02abs"
   },
   "outputs": [],
   "source": [
    "student_model.to(device)\n",
    "student_model.eval()\n",
    "\n",
    "# Creating dataloaders for test\n",
    "batch_size = 32\n",
    "num_cpu = 12\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "all_preds.clear()\n",
    "all_labels.clear()\n",
    "\n",
    "with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            student_outputs = student_model(inputs, attention_mask=attention_masks)\n",
    "            _, preds = torch.max(student_outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ],
   "id": "RY1HWfx02abs"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Gradient Clipping Technique"
   ],
   "metadata": {
    "id": "4nZSjcEr1uHR"
   },
   "id": "4nZSjcEr1uHR"
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR"
   ],
   "metadata": {
    "id": "cC552IE4Vs7R"
   },
   "execution_count": null,
   "outputs": [],
   "id": "cC552IE4Vs7R"
  },
  {
   "cell_type": "code",
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# declaring Hyperparameters\n",
    "batch_size = 32\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "num_cpu = 12\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "train_losses.clear()\n",
    "val_accuracies.clear()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "lambda_lr = lambda epoch: 0.90 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "\n",
    "# Creating data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for inputs, attention_masks, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks,labels =labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "   # Validation\n",
    "    model.eval()  # Setting model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model's predictions\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Printing the loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Saving the trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBGC.pt\"\n",
    "torch.save(best_model_state, BERT_Tuned_file)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d_1UvGQ1uHR",
    "outputId": "e3194c12-c5ff-43f4-c211-ae5008f77cf4"
   },
   "execution_count": null,
   "outputs": [],
   "id": "8d_1UvGQ1uHR"
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting epoch loss and validation accuracy graphs\n",
    "num_epochs = len(train_losses)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/BBGC_plot.png\"\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "WqsXQFT41uHS",
    "outputId": "d8299382-9725-46d2-a9b4-5ead82559c48"
   },
   "execution_count": null,
   "outputs": [],
   "id": "WqsXQFT41uHS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCGPCHcs1uHS"
   },
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBGC.pt\"\n",
    "model.load_state_dict(torch.load(BERT_Tuned_file))"
   ],
   "id": "nCGPCHcs1uHS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d4e79107-eecc-4591-fe26-5c691c648750",
    "id": "gR_q7DEM1uHS"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Creating dataloaders for test\n",
    "batch_size = 32\n",
    "num_cpu = 12\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "all_preds.clear()\n",
    "all_labels.clear()\n",
    "\n",
    "with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ],
   "id": "gR_q7DEM1uHS"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.1 Reducing Gradient Clipping"
   ],
   "metadata": {
    "id": "US3Se5La1uHS"
   },
   "id": "US3Se5La1uHS"
  },
  {
   "cell_type": "code",
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# declaring Hyperparameters\n",
    "batch_size = 32\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "num_cpu = 12\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "train_losses.clear()\n",
    "val_accuracies.clear()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "lambda_lr = lambda epoch: 0.90 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "\n",
    "# Creating data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for inputs, attention_masks, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks, labels = labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "   # Validation\n",
    "    model.eval()  # Setting model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model's predictions\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Printing the loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Saving the trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBGC1.pt\"\n",
    "torch.save(best_model_state, BERT_Tuned_file)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDiO_DNU1uHS",
    "outputId": "1784e6c0-3a3c-491e-fd2e-957e741144fe"
   },
   "execution_count": null,
   "outputs": [],
   "id": "xDiO_DNU1uHS"
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting epoch loss and validation accuracy graphs\n",
    "num_epochs = len(train_losses)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/BBGC1_plot.png\"\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "YE9wr0Y21uHS",
    "outputId": "0b8dd94d-f108-4a42-859e-132fda6b687e"
   },
   "execution_count": null,
   "outputs": [],
   "id": "YE9wr0Y21uHS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaqEFA181uHT"
   },
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBGC1.pt\"\n",
    "model.load_state_dict(torch.load(BERT_Tuned_file))"
   ],
   "id": "BaqEFA181uHT"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6fa9c196-8b69-432f-8de6-b901c5339c9a",
    "id": "TqOtwscD1uHT"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Creating dataloaders for test\n",
    "batch_size = 32\n",
    "num_cpu = 12\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "all_preds.clear()\n",
    "all_labels.clear()\n",
    "\n",
    "with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ],
   "id": "TqOtwscD1uHT"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.2 Further Reducing Gradient Clipping"
   ],
   "metadata": {
    "id": "iSD3gcPVJxOU"
   },
   "id": "iSD3gcPVJxOU"
  },
  {
   "cell_type": "code",
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# declaring Hyperparameters\n",
    "batch_size = 32\n",
    "early_stopping_patience = 3\n",
    "epochs_without_improvement = 0\n",
    "num_cpu = 12\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "train_losses.clear()\n",
    "val_accuracies.clear()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "lambda_lr = lambda epoch: 0.90 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "\n",
    "\n",
    "# Creating data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for inputs, attention_masks, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_masks, labels = labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.55)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "   # Validation\n",
    "    model.eval()  # Setting model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get model's predictions\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        epochs_without_improvement = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Printing the loss and accuracy for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Saving the trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBGC2.pt\"\n",
    "torch.save(best_model_state, BERT_Tuned_file)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "698c82ed-eb39-4913-b630-a5d6a4455740",
    "id": "P_N-35q2JxOU"
   },
   "execution_count": null,
   "outputs": [],
   "id": "P_N-35q2JxOU"
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting epoch loss and validation accuracy graphs\n",
    "num_epochs = len(train_losses)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/BBGC2_plot.png\"\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "outputId": "d6cbd424-3c08-439b-9cf7-f55b9d6d3853",
    "id": "ERcGhYtIJxOV"
   },
   "execution_count": null,
   "outputs": [],
   "id": "ERcGhYtIJxOV"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWZyYH7OJxOV"
   },
   "outputs": [],
   "source": [
    "# Loading trained model\n",
    "BERT_Tuned_file = \"/content/drive/MyDrive/Colab Notebooks/BBGC2.pt\"\n",
    "model.load_state_dict(torch.load(BERT_Tuned_file))"
   ],
   "id": "UWZyYH7OJxOV"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PF0kjOsCJxOV",
    "outputId": "b6c7f1b6-7922-49e1-926e-16d8ce76f200"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Creating dataloaders for test\n",
    "batch_size = 32\n",
    "num_cpu = 12\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_cpu, pin_memory=True, shuffle=False)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "all_preds.clear()\n",
    "all_labels.clear()\n",
    "\n",
    "with torch.no_grad():\n",
    "        for inputs, attention_masks, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask=attention_masks)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "class_report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ],
   "id": "PF0kjOsCJxOV"
  },
  {
   "cell_type": "code",
   "source": [
    "user_input = input(\"Enter a sentence: \")\n",
    "preprocessed_input = preprocess_text(user_input)\n",
    "encoded_input= tokenizer.batch_encode_plus(\n",
    "    [preprocessed_input],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "input_ids = encoded_input['input_ids'].to(device)\n",
    "attention_masks_test = encoded_input['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_masks_test)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "predicted_sentiment = class_names[predicted_label]\n",
    "print(\"Predicted sentiment:\", predicted_sentiment)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTkADjqcFgyM",
    "outputId": "479dc0c1-2fbc-4339-f5e2-ad74a6ae0171"
   },
   "id": "xTkADjqcFgyM",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "66b47aafb57949d793b470ed816cd405": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc6e906e176946669281ef9ca4a32562",
       "IPY_MODEL_b13161aab05347a4a8a8fb9840e772ca",
       "IPY_MODEL_9dcb9e4253744b75a0e52c679c07863f"
      ],
      "layout": "IPY_MODEL_1ab72a3125b54109b4a60903e08c3240"
     }
    },
    "dc6e906e176946669281ef9ca4a32562": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_544e3dd7ed164d36bac8f7c20670d103",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_eef0915666b74465a9b78ddd0d849d7b",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "b13161aab05347a4a8a8fb9840e772ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc206d92fd794419ab80d569dd8713cd",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f460075cf42e41bfb17b1ddfd2fb283f",
      "value": 440449768
     }
    },
    "9dcb9e4253744b75a0e52c679c07863f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7063fd0e3e744e2cb43d45cd310e2fc9",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_fdd966b9c1934687b3d577723ad5fa8d",
      "value": " 440M/440M [00:00&lt;00:00, 498MB/s]"
     }
    },
    "1ab72a3125b54109b4a60903e08c3240": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "544e3dd7ed164d36bac8f7c20670d103": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eef0915666b74465a9b78ddd0d849d7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc206d92fd794419ab80d569dd8713cd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f460075cf42e41bfb17b1ddfd2fb283f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7063fd0e3e744e2cb43d45cd310e2fc9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdd966b9c1934687b3d577723ad5fa8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}